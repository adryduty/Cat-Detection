{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adryduty/computer-vision-cat-project/blob/main/Real_Time_Detection_24_June.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = red> Real Time Detection"
      ],
      "metadata": {
        "id": "pzX5HfUYmJ6v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = blue> Import the necessary modules"
      ],
      "metadata": {
        "id": "5Q9qIsUvz4Aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from base64 import b64decode, b64encode\n",
        "import numpy as np \n",
        "import cv2\n",
        "from IPython.display import display, Javascript, Audio\n",
        "from google.colab.output import eval_js\n",
        "import io\n",
        "from PIL import Image\n",
        "import html"
      ],
      "metadata": {
        "id": "Q_YCbRYFiyah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae5thi3v1yPC"
      },
      "source": [
        "## <font color = blue> In the following code we build a YOLOv5s with the weights you have already stored (best.pt) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hLpckZt1v_T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b1ae4e9-e7b7-4773-c1f8-c2f99fc7da60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m PyYAML>=5.3.1 not found and is required by YOLOv5, attempting auto-update...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyYAML>=5.3.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "Installing collected packages: PyYAML\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-6.0\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per /root/.cache/torch/hub/ultralytics_yolov5_master/requirements.txt\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "YOLOv5 üöÄ 2022-6-24 Python-3.7.13 torch-1.11.0+cu113 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 213 layers, 7012822 parameters, 0 gradients\n",
            "Adding AutoShape... \n"
          ]
        }
      ],
      "source": [
        "model = torch.hub.load('ultralytics/yolov5', 'custom', \n",
        "                       path='/content/best.pt', force_reload=True)\n",
        "\n",
        "#model = torch.hub.load('ultralytics/yolov5', 'yolov5s') # This is pretrained (200000 pictures)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYCfgiYY1BNl"
      },
      "source": [
        "## <font color = blue> Let's use the camera and the frozen weights to detect Ghiri"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The following function takes an image from the webcam as input, encodes it in numpy array format and then decode it into an OpenCV BGR image. This image will be given to the model, in order to detect Ghiri."
      ],
      "metadata": {
        "id": "g2vXmvcGr7ve"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3-Lk1poIXz5"
      },
      "outputs": [],
      "source": [
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The following function converts an OpenCV rectangle bounding box image into base64 byte string to be overlayed on video stream. It'll be essential in order to display the red bounding box on the image"
      ],
      "metadata": {
        "id": "Y00v5Tv9r_jc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  This function converts OpenCV rectangle bounding box image \n",
        "  into base64 byte string to be overlayed on video stream\n",
        "\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ],
      "metadata": {
        "id": "x-AlelLFsAeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The following function uses Javascript to properly create our live video stream using the webcam as input.\n",
        "\n"
      ],
      "metadata": {
        "id": "rMSsmXMMoLAV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTuD3OUG1H4c"
      },
      "outputs": [],
      "source": [
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The following function works in conjunction with video_stream. Basically it takes the label and the bbox (initialized to be an empty string) and returns a dictionary with different indices (create, show, capture and img). We are just interested in the values associated with the key 'img' which is the actual frame which we will pass to the js_to_image function. "
      ],
      "metadata": {
        "id": "qvrW81hJxgN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def video_frame(label, bbox):\n",
        "  '''\n",
        "  This function takes label and bbox and returns an image in base64 format with the bounding box you pass to it.\n",
        "  '''\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "metadata": {
        "id": "uzxf3Pq_tdlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D1gzBuVGWSN"
      },
      "source": [
        "### Running on Webcam Video"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "\n",
        "count = 0 \n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    frame = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    frame = Image.fromarray(frame)\n",
        "    results = model(frame)\n",
        "\n",
        "    df_results = results.pandas().xywh[0].sort_values(by = [\"confidence\"], ascending=False) #saves the results in a pandas df, ordered from higher confidence to lower\n",
        "    # Notice that if you use results.xywh instead of results.xywhn, you have as result x_c, y_c, width, height, conf not standardized.\n",
        "\n",
        "    # create transparent overlay for bounding box\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8) #RGBA format (4th channel is for extra informations)\n",
        "\n",
        "\n",
        "    if not df_results.empty:               #check if results df is empty or not; if it's empty no bbox needs to be drawed\n",
        "      x_c, y_c, width, height, conf = df_results.iloc[0,:5].astype(float) #assign to each variable the proper value taken from the best result (row 0)\n",
        "\n",
        "      #convert bbox coordinates in a system suitable for cv2\n",
        "      left = int(x_c - width/2)\n",
        "      top = int(y_c - height/2)\n",
        "      right = int(x_c + width/2)\n",
        "      bottom = int(y_c + height/2)\n",
        "\n",
        "      bbox_array = cv2.rectangle(bbox_array, (left, top), (right, bottom), (255,0,0), 2) #create bounding box\n",
        "      bbox_array = cv2.putText(bbox_array, \"{} [{:.2f}]\".format(\"Ghiri\", conf),          #label boundig box with label name and confidence\n",
        "                          (left, top - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
        "                          (255,0,0), 2)\n",
        "\n",
        "      bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255             #4th layer (3 considering that starts from 0), for the transparency\n",
        "\n",
        "      # convert overlay of bbox into bytes\n",
        "      bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "\n",
        "      if conf > 0.7:\n",
        "        sound_file = \"/content/beep.mp3\"\n",
        "        display(Audio(sound_file, autoplay=True))\n",
        "\n",
        "      # update bbox so next frame gets new overlay\n",
        "      bbox = bbox_bytes"
      ],
      "metadata": {
        "id": "3iJgfEIn9vd1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ae5thi3v1yPC"
      ],
      "name": "Real_Time_Detection_24_June.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOPaloWGsKXSf6n7W1BTbrv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}